# -*- coding: utf-8 -*-
"""IMDb .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jNPJMHVwd0UV5v9q2cM9Crux3W-5X5U4
"""

import json

from huggingface_hub import login
login("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", add_to_git_credential=True)

from datasets import load_dataset
dataset = load_dataset("imdb")

from transformers import AutoTokenizer, AutoModelForSequenceClassification
tokenizer = AutoTokenizer.from_pretrained("n-base-uncased")
model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased")

dataset

def tokenize_function(example):
  return tokenizer(example["text"], padding="max_length", truncation=True)

tokenized_dataset = dataset.map(tokenize_function, batched=True)

# datasets

from transformers import TrainingArguments, Trainer
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    # learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
)

import torch
print(torch.cuda.is_available())  #

print(model.device)

trainer.train()

# save info

trainer.save_model("./results")
tokenizer.save_pretrained("./results")
trainer.state.save_to_json("./results/trainer_state.json")

eval_results = trainer.evaluate()
print(eval_results)

with open("./results/eval_results.json", "w") as f:
  json.dump(eval_results, f)

trainer.push_to_hub("imdb_text_classification")

"""**examples**"""

# Example input
input_text = "For most of the book, I felt as dumbfounded as I would have been if I were browsing through a psychiatric journal. Filled with references and technical terms and statistics, it was mostly a book-long affirmation of the then innovative technique called 'logo-therapy'. I do not understand how this book is still relevant and found in most popular book stores. It might have been that the book was popular in the sixties and seventies as it offered a powerful and logical argument against the reductionist approach that leads inevitably to existential nihilism, but is that still relevant today? It also attempts to free psychiatry from the belief that 'eros' was the cause of all neurosis and turns the flashlight on repressed 'logos' - which forms the premise of the book and the title."

# Tokenize the input text
inputs = tokenizer(input_text, return_tensors="pt", truncation=True, padding=True)

# inputs and model should be on the same device
import torch
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Move input tensors to the same device as the model
inputs = {k: v.to(device) for k, v in inputs.items()}  # Move inputs to GPU

# Ensure model is on GPU
model.to(device)

model.eval()

with torch.no_grad():
  outputs = model(**inputs)

logits = outputs.logits
predicted_class = torch.argmax(logits, dim=-1).item()